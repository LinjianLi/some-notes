{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Notes About AI, ML and DL\n",
    "\n",
    "写这个的最初目的是用于大二期末考试的复习。从课堂、书、网络教程整理下来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "The smaller, the better.\n",
    "\n",
    "### Negative Log Likelihood (NLL)\n",
    "\n",
    "$$\n",
    "- log(p(y))\n",
    "$$\n",
    "\n",
    "### Focal Loss\n",
    "\n",
    "The loss will be larger when the classifier is not confident, that is, when $p(y)$ is small.\n",
    "\n",
    "$$\n",
    "- (1 - p(y))^{\\gamma} \\cdot log(p(y))\n",
    "$$\n",
    "\n",
    "### Cross Entropy\n",
    "\n",
    "(Usually for classification.)\n",
    "\n",
    "$$\n",
    "- \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{K} \\mathbb{1}(y_{i} = j) \\cdot log\\left(p(\\hat{y_{i}}=j)\\right)\n",
    "$$\n",
    "\n",
    "Note: KL-divergence is very similar to cross entropy but not the same. Suppose we have two distributions, $P$ and $Q$. Cross entropy measures average number of **total** bits to represent an event from $Q$ instead of $P$. KL-divergence measures average number of **extra** bits to represent an event from $Q$ instead of $P$.\n",
    "\n",
    "### Mean Absolute Error (MAE, L1)\n",
    "\n",
    "(Usually for regression.)\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} \\lvert y_{i} - \\hat{y_{i}} \\rvert\n",
    "$$\n",
    "\n",
    "### Mean Square Error (MSE, L2)\n",
    "\n",
    "(Usually for regression.)\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^{n} (y_{i} - \\hat{y_{i}})^2\n",
    "$$\n",
    "\n",
    "### Hinge Loss\n",
    "\n",
    "$$\n",
    "\\xi_{i} = max \\big( 0, 1-y\\hat{y} \\big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 宋恒杰课堂记录\n",
    "\n",
    "AI 与 ML 最大的差别在于后者用到了 probabilistic information 或 statistics information，而前者没有。因此进化学习、传统神经网络都不属于 ML，他们属于 AI。\n",
    "\n",
    "神经网络不能保证收敛。\n",
    "\n",
    "### 决策树\n",
    "\n",
    "优点:\n",
    "\n",
    "1. 清晰的可解释性\n",
    "2. 非线性关系隐含在简单树形结构中\n",
    "3. 少量的缺失数据对模型效果影响不大\n",
    "\n",
    "训练决策树的关键问题是判断 feature 的顺序(重要性)。每轮 iteration 都要重新计算余下的所有 feature 的信息增益。不能一次计算完然后排序然后结束。\n",
    "\n",
    "C4.5 优点: 能并行处理数据。\n",
    "    \n",
    "### 模糊神经网络\n",
    "\n",
    "1. 给定输入，若模糊子集个数确定，则规则子集的个数也随之确定\n",
    "2. 第一、二层非全连接，因此训练工作量减少\n",
    "3. 不用训练连接的权重值，只需训练神经元的相关参数\n",
    "\n",
    "\n",
    "### 深度学习\n",
    "\n",
    "用来进行 feature learning\n",
    "\n",
    "通过学习一组可见即可得的特征，得到一组可能不具备特定物理含义但却对 inference 的准确度有提高的特征。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "更新参数应该是同步的，即\n",
    "\n",
    "correct:\n",
    "\n",
    "$$\n",
    "temp_1 = \\theta_1 - \\alpha \\frac{\\partial J}{\\partial \\theta_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "temp_2 = \\theta_2 - \\alpha \\frac{\\partial J}{\\partial \\theta_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_1 = temp_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_2 = temp_2\n",
    "$$\n",
    "\n",
    "wrong:\n",
    "\n",
    "$$\n",
    "\\theta_1 = \\theta_1 - \\alpha \\frac{\\partial J}{\\partial \\theta_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_2 = \\theta_2 - \\alpha \\frac{\\partial J}{\\partial \\theta_2}\n",
    "$$\n",
    "\n",
    "In the wrong example, the update of  $\\theta_1$ will affect the update of $\\theta_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "MSE (L2) loss function.\n",
    "\n",
    "$$\n",
    "L(w) = \\frac{1}{2} \\Vert y-(wX + b) \\Vert _{2}^{2}\n",
    "$$\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "### Closed-form Solotion\n",
    "\n",
    "$$w^* = argmin_w L(w) = (X^{T} X)^{-1} X^{T} y$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "$$\n",
    "g(z) = \\big( 1+exp(-z) \\big)^{-1}\n",
    "$$\n",
    "$$\n",
    "g'(z) = g(z) \\cdot \\big( 1-g(z) \\big)\n",
    "$$\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "Minimize binary cross-entropy.\n",
    "\n",
    "$$\n",
    "h_{w}(x) = g(w^{T} x) = P(y = +1 | x)\n",
    "$$\n",
    "\n",
    "$$\n",
    "L(w) = - \\frac{1}{n} \\sum_{i=1}^{n} \\bigg[ y_{i} \\cdot log \\big( h_w(x_{i}) \\big) + (1-y_{i}) \\cdot log \\big( 1-h_{w}(x_{i}) \\big) \\bigg]\n",
    "$$\n",
    "\n",
    "assume $y \\in \\{0,1\\}$\n",
    "\n",
    "### Gradient\n",
    "\n",
    "For one instance:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w)}{\\partial w} = \\big( h_{w}(x)-y \\big) x\n",
    "$$\n",
    "\n",
    "For all instances:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w)}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} \\big( h_{w}(x_{i})-y_{i} \\big) x_{i}\n",
    "$$\n"
   ]
  },
  {
   "source": [
    "## Softmax Regression\n",
    "\n",
    "General form of logistic regression, k possible outcomes instead of 2. Minimize cross-entropy.\n",
    "\n",
    "$$\n",
    "P(y_{i} = j | x) = \\frac{exp(w_{j}^{T} x_{i})}{\\sum_{l=1}^{K} exp(w_{l}^{T} x_{i})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L(w) = - \\frac{1}{n} \\bigg[ \\sum_{i=1}^{n} \\sum_{j=1}^{K} \\mathbb{1}(y_{i} = j) log(P(y_{i} = j | x)) \\bigg]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w)}{\\partial w_{j}} = \\frac{1}{n} \\sum_{i=1}^{n} \\bigg[ \\big( P(y_{i} = j | x_{i} ; w) - \\mathbb{1}(y_{i} = j) \\big) x_{i} \\bigg] + \\lambda w_{j}\n",
    "$$\n",
    "\n",
    "where $\\mathbb{1}()$ is the indicator function such that $\\mathbb{1}(true)=1$ and $\\mathbb{1}(false)=0$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vecotor Machine (SVM)\n",
    "\n",
    "### Some Keywords\n",
    "\n",
    "Maximize the margin, which is to minimize the L2-norm of coefficients $w$.\n",
    "\n",
    "Lagrange, primal and dual problem, Krause-Kuhn-Tucker (KKT) conditions, support vectors\n",
    "\n",
    "feature mapping, kernels, Gaussian kernel, kernel matrix\n",
    "\n",
    "regularization, allow some instances have margin of $1 - \\xi_{i}$, the cost is $C \\xi_{i}$\n",
    "\n",
    "### Objective\n",
    "\n",
    "Distance of point to line:\n",
    "\n",
    "$$\n",
    "\\frac{|Ax+By+C|}{\\sqrt{A^{2}+B^{2}}} = \\frac{|\\overrightarrow{(A, B)} \\cdot \\overrightarrow{(x, y)}+C|}{\\sqrt{A^{2}+B^{2}}} = \\frac{|wx+b|}{||w||_{2}}\n",
    "$$\n",
    "\n",
    "To maximize the distance\n",
    "\n",
    "$$\n",
    "max_{w,b} \\frac{|wx+b|}{||w||_{2}}\n",
    "$$\n",
    "\n",
    "is equivalent to\n",
    "\n",
    "$$\n",
    "min_{w, b} \\frac{||w||_{2}^{2}}{2}\n",
    "$$\n",
    "\n",
    "s.t.\n",
    "\n",
    "$$\n",
    "y^{(i)} (w^T x^{(i)} + b) \\geq 1\n",
    "$$\n",
    "\n",
    "which is\n",
    "\n",
    "$$\n",
    "g_{i}(w) \\triangleq 1 - y^{(i)} (w^{T} x^{(i)} + b) \\leq 0\n",
    "$$\n",
    "\n",
    "(assume $y \\in \\{-1, +1\\}$)\n",
    "\n",
    "### Lagrangian for Optimization Problem\n",
    "\n",
    "$$\n",
    "L(w, b, \\alpha)=\\frac{||w||_2^2}{2} - \\sum_{i=1}^{m}\\alpha_{i}[g_{i}(w)] \\qquad (\\textrm{s.t.} \\: \\alpha_{i} \\geq 0)\n",
    "$$\n",
    "\n",
    "### Dual form of the optimization problem\n",
    "\n",
    "$$\n",
    "MaxMin \\leq MinMax\n",
    "$$\n",
    "\n",
    "### Kernel Trick\n",
    "\n",
    "Question: $K(x, z)$ is defined using $\\phi(x)$ and $\\phi(z)$. How can we represent or calculate $K$ without knowing $\\phi$?\n",
    "\n",
    "Answer: We don't care about what $\\phi(x)$ and $\\phi(z)$ are. We just want to classify the instances, no matter what space they are projected to.\n",
    "\n",
    "### Soft Margin\n",
    "\n",
    "$$\n",
    "min_{w, b} \\frac{||w||^2}{2} + \\frac{C}{n} \\ \\sum_{i=1}^{n} \\xi_{i}\n",
    "$$\n",
    "\n",
    "s.t.\n",
    "\n",
    "$$\n",
    "y^{(i)} (w^T x^{(i)} + b) \\geq 1- \\xi_{i}\n",
    "$$\n",
    "\n",
    "#### Hinge Loss\n",
    "\n",
    "$$\n",
    "\\xi_{i} = max \\big( 0, 1-y^{(i)} (w^{T} x^{(i)} + b) \\big)\n",
    "$$\n",
    "\n",
    "### SMO (sequential minimal optimization)\n",
    "\n",
    "Coordinate ascent/descent. Select a pair of features, instead of one feature, at each iteration. The reason for selecting a pair is the constraint that the sum is zero.\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. 不会自己学习feature，要手动选择feature作为input\n",
    "2. 训练完成后，保存model时，要存training data (support vector)\n",
    "3. 所有数据要存在单台设备上才能做SMO凸优化\n",
    "\n",
    "#### Deal with Disadvantages\n",
    "\n",
    "1. feature engineering，特征工程\n",
    "2. artificial support vector\n",
    "3. (I forget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "### Entropy and information gain\n",
    "\n",
    "The purity of the node:\n",
    "\n",
    "$$\n",
    "Entropy(D) = \\sum_{i=1}^{c} -p_{i} log_{2} p_{i}\n",
    "$$\n",
    "\n",
    "Information gain:\n",
    "\n",
    "$$\n",
    "Gain(D, A) = Entropy(D) - \\sum_{v \\in Values(A)} \\frac{|D_{v}|}{D} Entropy(D_{v})\n",
    "$$\n",
    "\n",
    "Gain raito:\n",
    "\n",
    "$$\n",
    "GainRatio(D, A) = \\frac{Gain(D, A)}{IV(A)}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "IV(A) = - \\sum_{v \\in Values(A)} \\frac{|D_{v}|}{D} log_{2} \\frac{|D_{v}|}{D}\n",
    "$$\n",
    "\n",
    "is the intrinsic value of the attribute $A$.\n",
    "\n",
    "## Random forest\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Ensemble learning, Bootstrapped Aggregation (Bagging), XGBoost, Gradient boosting machine (GBM), Gradient boosting decision tree (GBDT)\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "Base (weak) classifier, weights, error rate\n",
    "\n",
    "$$w_{m+1}(i) = \\frac{w_{m}(i)}{z_{m}} exp\\big(-\\alpha_{m} y_{i} h_{m}(x_{i})\\big)$$\n",
    "\n",
    "$$z_{m} = \\sum_{i-1}^{n} exp\\big(-\\alpha_{m} y_{i} h_{m}(x_{i})\\big)$$\n",
    "\n",
    "$$\\alpha_{m} = \\frac{1}{2} log \\frac{1-\\epsilon_{m}}{\\epsilon_{m}}$$\n",
    "\n",
    "$$H(x) = \\sum_{m=1}^{M} \\alpha_m h_m(x)$$\n",
    "\n",
    "### Gradient Boosting Decision Tree (GBDT)\n",
    "\n",
    "Train multiple decision trees to predict the residual. The final prediction is the sum of the learning rate times the prediction of each decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Text classification, independence assumption, Laplace smoothing\n",
    "\n",
    "## Bayesian network\n",
    "\n",
    "Chow-Liu tree, exact inference, Junction tree, variable elimination, belief propagation, approximate inference, sampling, structural learning, scoring function,\n",
    "\n",
    "## K-Nearest neighbours (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation-Maximization (EM)\n",
    "\n",
    "Two steps\n",
    "\n",
    "- E-step: Estimate the missing variables in the dataset.\n",
    "- M-step: Maximize the parameters of the model in the presence of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "### K-means\n",
    "\n",
    "Belongs to EM. K is the hyperparameter selected by hand.\n",
    "\n",
    "#### Objective\n",
    "\n",
    "If use one-hot encoding\n",
    "\n",
    "$$\n",
    "L(r, \\mu) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} r_{ik} ||x_{i} - \\mu_{k}||\n",
    "$$\n",
    "\n",
    "Expectation-Maximization (EM)\n",
    "\n",
    "- E-step: $\\mu_{k} = \\frac{1}{n_{k}} \\sum_{i=1}^{n} r_{ik}x_{i}$\n",
    "- M-step: $r_{i} = argmin_{k} ||x_{i} - \\mu_{k}||$\n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "**D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise.\n",
    "\n",
    "First, find all core objects.\n",
    "\n",
    "Similar to finding the all connected components by depth first search (DFS) using a queue. The connection is defined by “density-reachable”. The starting points are core objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis (PCA)\n",
    "\n",
    "基于 Mingkui Tan 的课件。\n",
    "\n",
    "原本是向量 $x$，找到一个矩阵 $W$，使得新向量 $z$ 满足 $z = Wx$。相当于旋转坐标系。$z, x$ 的维度相同。\n",
    "\n",
    "$z$ 的每一维度的重要性是递减的，即 $z_{1}$ 的重要性大于 $z_{2}$ 大于 $z_{3}$ 大于......\n",
    "\n",
    "$z_{1}$ 是最重要的维，$z_{2}$ 是第二重要的维......\n",
    "\n",
    "$w^{1}$，也就是 $W$ 的第 1 行，把 $x$ 投影到 $z_{1}$ 上，也就是 $z$ 的第 1 维。其他 $w^i$ 同理\n",
    "\n",
    "假设现在我们只关注***最重要的 1 维 $z_{1}$***。\n",
    "\n",
    "对任意一个 $x$，要把 $x$ 投影到这 1 维上，那么 $z_{1} = w^{1} x$。**如何找到这个 $w^1$？**\n",
    "\n",
    "We want the variance of $z_{1}$ as large as possible. So,\n",
    "\n",
    "$$w^1 = argmax \\: Var(z_{1}) = \\frac{1}{N} \\sum_{z_{1}}(z_{1}-\\bar{z_{1}})^2$$\n",
    "\n",
    "subject to $||w^1||_2 = 1$\n",
    "\n",
    "那么 $Var(z_{1})$ 是什么？\n",
    "\n",
    "看复习 PPT 的第 10 页，最后导出 $$Var(z_{1}) = (w^{1})^T Cov(x) (w^{1})$$。把 $Cov(x)$ 记为S。\n",
    "\n",
    "So,\n",
    "\n",
    "$$w^{1} = argmax \\big((w^{1})^{T} S (w^{1})\\big)$$\n",
    "subject to $||w^{1}||_{2} = 1$\n",
    "\n",
    "有约束的优化问题就可以用拉格朗日乘子法来解。PPT 第 11 页的 $\\alpha$ 就是 Lagrange multiplier。\n",
    "各项求导并令其为零就得到 $Sw^{1} - \\alpha w^{1} = 0$，于是得到红色的等式 $Sw^{1} = \\alpha w^{1}$，左右同时左乘一个 $(w^{1})^{T}$ 就得到了 $(w^{1})^{T} S (w^{1}) = \\alpha$。联系前面的等式，so，\n",
    "\n",
    "$$ w^{1} = argmax(\\alpha) $$\n",
    "\n",
    "所以 choose the maximum one for $\\alpha$。$w^1$就是 $S$ 的特征向量，对应 $S$ 的最大特征值 $\\lambda_{1}$。\n",
    "\n",
    "同理，如果我们想要找到 $w^{2}$，那它也是 $S$ 的特征向量，对应 $S$ 的第二大特征值 $\\lambda_{2}$。同时，$w^{1}$ 与 $w^{2}$ 应该是正交的，相乘为零。\n",
    "\n",
    "The eigenvalues for symmetric matrices are always real. A symmetric n×n real matrix M is said to be positive definite if the scalar $z^{T}Mz$ is positive for every non-zero column vector z of n real numbers.\n",
    "$z^{T}Mz \\geq 0$\n",
    "\n",
    "至于怎么算特征值和特征向量？\n",
    "\n",
    "***忘了***\n",
    "\n",
    "不过WQY说只考概念，所以有可能不用计算？"
   ]
  },
  {
   "source": [
    "## Recommender System\n",
    "\n",
    "### Model-based Collaborative Filtering\n",
    "\n",
    "m: Number of users\n",
    "\n",
    "n: Number of items\n",
    "\n",
    "k: Number of feature\n",
    "\n",
    "R is m-by-n matrix, P is m-by-k user matrix, Q is k-by-n item matrix.R=PQ\n",
    "\n",
    "Suppose $p$ and $q$ are k-by-1 matries, which are k-dimensional **column** vectors.\n",
    "\n",
    "$$P = [p_{1}, p_{2}, ..., p_{m}]^{T}$$\n",
    "\n",
    "$$Q = [q_{1}, q_{2}, ..., q_{m}]$$\n",
    "\n",
    "Prediction of one element:\n",
    "\n",
    "$$\\hat{R_{ui}} = P_{u\\cdot} Q_{\\cdot i} = (p_{u})^{T} q_{i}$$\n",
    "\n",
    "Loss for one element (squared error loss):\n",
    "\n",
    "$$L(R_{ui},\\hat{R_{ui}}) = (R_{ui}-\\hat{R_{ui}})^{2} = (R_{ui}-(p_{u})^{T} q_{i})^{2}$$\n",
    "\n",
    "Loss for the whole P and Q is the sum of loss of each element, plus regulation part:\n",
    "\n",
    "$$L = \\sum_{u,i}(R_{ui}-(p_{u})^{T} q_{i})^{2} + \\lambda ( \\sum_{u} n_{p_{u}} ||p_{u}||_{2}^{2} + \\sum_{i} n_{q_{i}} ||q_{i}||_{2}^{2} )$$\n",
    "\n",
    "### ALS\n",
    "\n",
    "Fixing Q, optimize P, update each $p_{u}$ as:\n",
    "\n",
    "$$p_{u} \\gets \\sum_{i} (q_{i} q_{i}^{T} + \\lambda n_{p_{u}}I)^{-1} Q^{T} R_{u \\cdot}^{T}$$\n",
    "\n",
    "Fixing P, optimize Q, update each $q_{i}$ as:\n",
    "\n",
    "$$q_{i} \\gets \\sum_{u} (p_{u} p_{u}^{T} + \\lambda n_{q_{i}}I)^{-1} P^{T} R_{\\cdot i}$$\n",
    "\n",
    "ALS is **NOT** scalable to large-scale datasets, but SGD is.\n",
    "\n",
    "### SGD\n",
    "\n",
    "SGD choose the loss function as: $L = \\sum_{u,i}(R_{ui}-(p_{u})^{T} q_{i})^{2} + ( \\lambda_p ||p_{u}||_{2}^{2} + \\lambda_q ||q_{i}||_{2}^{2} )$\n",
    "\n",
    "- $ E_{ui} = R_{ui} - (p_{u})^{T} q_{i} $\n",
    "- $ \\frac{\\partial L}{\\partial p_{u}} = E_{ui}(-q_{i}) + \\lambda_{p} p_{u} $\n",
    "- $ \\frac{\\partial L}{\\partial q_{i}} = E_{ui}(-p_{u}) + \\lambda_{q} q_{i} $"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent components analysis (ICA)\n",
    "\n",
    "## Reinforcement learning\n",
    "\n",
    "## Neural network and related concepts\n",
    "\n",
    "CNN,  \n",
    "RNN, LSTM,  \n",
    "BP, ReLU  \n",
    "GNN  \n",
    "Auto-encoder  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN).\n",
    "\n",
    "### Sequence to Sequence\n",
    "\n",
    "Encoder and decoder. Hidden states. Context vector (the encoder’s last hidden state).\n",
    "\n",
    "Input/output elements are characters or words (index). The indexes need to be changed to embeddings.\n",
    "\n",
    "Simple seq2seq does not need a maximum sentence length constraint, but seq2seq with **attention mechanism** needs one.\n",
    "\n",
    "[Pytorch tutorial for seq2seq](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "\n",
    "“***Teacher forcing***” is the concept of using the **real target** outputs as each next input, instead of using the **decoder’s guess** as the next input. Using teacher forcing causes it to converge faster but when the trained network is exploited, it may exhibit instability.\n",
    "\n",
    "**Beam search** is often applied to seq2seq model on test stage. It considers **multiple**, instead of the best one, solutions simultaneously at each step. The number of solutions considered simultaneously is controled by the parameter called **beam width**. If the beam width is set to 1, then the beam search is downgraded to greedy search. When implementing the algorithm, we usually use addition on logarithmic space instead of using multiplication on normal space, to avoid underflow. One of the disadvantages of beam search is that it tends to prefer shorter sentence, because the longer sentence tends to get lower probability. Here is [an article about beam search](https://medium.com/@dhartidhami/beam-search-in-seq2seq-model-7606d55b21a5). \n",
    "\n",
    "**Orthogonal initialization** is an approach to deal with the problem of gradient vanishing/expolding. Here is [an article about orthogonal initialization](https://medium.com/@dhartidhami/beam-search-in-seq2seq-model-7606d55b21a5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout 可以理解为让模型不要过分依赖某一个特征，因为这个特征随时可能被清除（dropout）。\n",
    "\n",
    "Dropout 的缺点是它的存在使得 loss function 不再是严格定义的了。严格定义的 loss function 在每次 iteration 肯定会往 loss 下降的方向走，引入 dropout 就是引入了随机性，就没法在每次 iteration 都保证 loss 下降。写程序的一个建议就是在调试的时候把 dropout 去掉或者概率设置为 0，然后看看是不是每个 iteration 的 loss 都在下降，是的话就说明模型至少是能正常、正确地工作，然后再设置自己想要的 dropout 概率进行训练。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}