{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Notes and Equations in AI, ML and DL\n",
    "\n",
    "写这个的最初目的是用于大二期末考试的复习。从课堂、书、网络教程整理下来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loss Functions\n",
    "\n",
    "### Cross Entropy\n",
    "\n",
    "(Usually for classification.)\n",
    "\n",
    "$- \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{K} \\mathbb{1}(y_i = j) \\cdot log\\left(p(\\hat{y_i}=j)\\right)$\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "(Usually for regression.)\n",
    "\n",
    "$\\frac{1}{n} \\sum_{i=1}^{n} \\lvert y_i - \\hat{y_i} \\rvert$\n",
    "\n",
    "### Mean Square Error (MSE)\n",
    "\n",
    "(Usually for regression.)\n",
    "\n",
    "$\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "AI与ML最大的差别在于后者用到了probabilistic information或statistics information，前者没有。因此进化学习、传统神经网络都不属于ML，他们属于AI。\n",
    "\n",
    "神经网络不能保证收敛。\n",
    "\n",
    "### 决策树\n",
    "优点：（1）清晰的可解释性。（2）非线性关系隐含在简单树形结构中。（3）少量的缺失数据对模型效果影响不大。<br/>\n",
    "训练决策树的关键问题是判断feature的顺序。<br/>\n",
    "每轮iteration都要重新计算余下的所有feature的信息增益。不能一次计算完然后排序然后结束。 <br/>\n",
    "C4.5优点：能并行处理数据。 <br/>\n",
    "    \n",
    "### 模糊神经网络\n",
    "（1）给定输入，若模糊子集个数确定，则规则子集的个数也随之确定。（2）第一、二层非全连接，因此训练工作量减少。（3）不用训练连接的权重值，只需训练神经元的相关参数。\n",
    "\n",
    "\n",
    "### 深度学习\n",
    "unsupervised learning <br/>\n",
    "用来进行feature learning <br/>\n",
    "通过学习一组可见即可得的特征，得到一组可能不具备特定物理含义但却对inference的准确度有提高的特征。\n",
    "\n",
    "#### Reference\n",
    "宋恒杰课堂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gradient Descent\n",
    "\n",
    "更新参数应该是同步的，即\n",
    "\n",
    "correct: <br/>\n",
    "$temp_1 = \\theta_1 - \\alpha \\frac{\\partial J}{\\partial \\theta_1}$ <br/>\n",
    "$temp_2 = \\theta_2 - \\alpha \\frac{\\partial J}{\\partial \\theta_2}$ <br/>\n",
    "$\\theta_1 = temp_1$ <br/>\n",
    "$\\theta_2 = temp_2$ <br/>\n",
    "\n",
    " wrong: <br/>\n",
    "$\\theta_1 = \\theta_1 - \\alpha \\frac{\\partial J}{\\partial \\theta_1}$ <br/>\n",
    "$\\theta_2 = \\theta_2 - \\alpha \\frac{\\partial J}{\\partial \\theta_2}$ <br/>\n",
    "\n",
    "In the wrong example, the update of  $\\theta_1$ will affect the update of $\\theta_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear Regression\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "### Closed-form Solotion\n",
    "* $ L(w) = \\frac{1}{2} \\Vert y-Xw \\Vert _2^2 $\n",
    "* $ w^* = argmin_w L(w) = (X^T X)^{-1} X^T y $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Logistic Regression\n",
    "### Sigmoid Function\n",
    "$ g(z) = \\big( 1+exp(-z) \\big)^{-1} $ <br/>\n",
    "$ g'(z) = g(z) \\cdot \\big( 1-g(z) \\big) $\n",
    "### Objective Function\n",
    "$ h_w(x) = g(w^T x) = P(y = +1 | x) $ <br/>\n",
    "$ J(w) = - \\frac{1}{n} \\bigg[ \\sum_{i=1}^{n} y_i log \\big( h_w(x_i) \\big) + (1-y_i) log \\big( 1-h_w(x_i) \\big) \\bigg] $ <br/>\n",
    "assume $ y \\in \\{0,1\\} $\n",
    "### Gradient\n",
    "For one instance: $ \\frac{\\partial J(w)}{\\partial w} = \\big( h_w(x)-y \\big) x $ <br/>\n",
    "For all instances: $ \\frac{\\partial J(w)}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} \\big( h_w(x_i)-y_i \\big) x_i $\n",
    "\n",
    "---\n",
    "## Softmax Regression\n",
    "\n",
    "General form of logistic regression, k possible outcomes instead of 2\n",
    "\n",
    "$ P(y_i = j | x) = \\frac{exp(w_j^T x_i)}{\\sum_{l=1}^{K} exp(w_l^T x_i)} $ <br/>\n",
    "$ J(w) = - \\frac{1}{n} \\bigg[ \\sum_{i=1}^{n} \\sum_{j=1}^{K} \\mathbb{1}(y_i = j) log(P(y_i = j | x)) \\bigg] $ <br/>\n",
    "$ \\frac{\\partial J(w)}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^{n} \\bigg[ \\big( P(y_i = j | x_i ; w) - \\mathbb{1}(y_i = j) \\big) x_i \\bigg] + \\lambda w_j $ <br/>\n",
    "\n",
    "where $\\mathbb{1}()$ is the indicator function such that $\\mathbb{1}(true)=1$ and $\\mathbb{1}(false)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Support Vecotor Machine (SVM)\n",
    "\n",
    "### Some Keywords\n",
    "\n",
    "Maximize the margin, which is minimize the L2-norm of coefficient\n",
    "\n",
    "Lagrange, primal and dual problem, Krause-Kuhn-Tucker (KKT) conditions, support vectors\n",
    "\n",
    "feature mapping, kernels, Gaussian kernel, kernel matrix\n",
    "\n",
    "regularization, allow some instances have margin of $1-\\xi_i$, the cost is $C\\xi_i$\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "$ min_{w,b} \\frac{||w||^2}{2} $ <br/>\n",
    "s.t. $ y^{(i)} (w^T x^{(i)} + b) \\geq 1 $\n",
    "\n",
    "(assume $ y \\in \\{-1,+1\\} $)\n",
    "\n",
    "### Lagrangian for Optimization Problem\n",
    "$L(w,b,\\alpha)=\\frac{1}{2}||w||_2^2 - \\sum_{i-1}^{m}\\alpha_{i}[y^{(i)}(w^{T}x^{(i)}+b)-1]$ <br/>\n",
    "s.t. $\\alpha_i\\geq0$\n",
    "\n",
    "### Dual form of the optimization problem\n",
    "\n",
    "\n",
    "### Soft Margin\n",
    "\n",
    "$ min_{w,b} \\frac{||w||^2}{2} + \\frac{C}{n} \\ \\sum_{i=1}^{n} \\xi_i $ <br/>\n",
    "s.t. $ y^{(i)} (w^T x^{(i)} + b) \\geq 1- \\xi_i $\n",
    "\n",
    "##### hinge loss \n",
    "$ \\xi_i = max \\big( 0,1-y^{(i)} (w^T x^{(i)} + b) \\big) $\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. 不会自己学习feature，要手动选择feature作为input\n",
    "2. 训练完成后，保存model时，要存training data（support vector）\n",
    "3. 所有数据要存在单台设备上才能做SMO凸优化\n",
    "\n",
    "#### Deal with Disadvantages\n",
    "\n",
    "1. feature engineering，特征工程\n",
    "2. artificial support vector\n",
    "3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SMO (sequential minimal optimization)\n",
    "Coordinate ascent/descent, select a pair, instead of one, at each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ensemble Learning\n",
    "\n",
    "## Decision Tree\n",
    "### Entropy and information gain\n",
    "$ Entropy(D) = \\sum_{i=1}^{c} -p_i log_2 p_i $ <br/>\n",
    "$ Gain(D,A) = Entropy(D)-\\sum_{v \\in Values(A)} \\frac{|D_v|}{D} Entropy(D_v) $ <br/>\n",
    "\n",
    "## Random forest\n",
    "\n",
    "## Boosting\n",
    "Ensemble learning, Bootstrapped Aggregation (Bagging), XGBoost, Gradient boosting machine (GBM), Gradient boosting decision tree (GBDT)\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "Base (weak) classifier, weights, error rate\n",
    "\n",
    "$w_{m+1}(i) = \\frac{w_{m}(i)}{z_m} exp\\big(-\\alpha_m y_i h_m(x_i)\\big)$\n",
    "\n",
    "$z_m = \\sum_{i-1}^{n} exp\\big(-\\alpha_m y_i h_m(x_i)\\big)$ <br/>\n",
    "$\\alpha_m = \\frac{1}{2} log \\frac{1-\\epsilon_m}{\\epsilon_m}$ <br/>\n",
    "$H(x) = \\sum_{m=1}^{M} \\alpha_m h_m(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "Text classification, independence assumption, Laplace smoothing\n",
    "    \n",
    "    \n",
    "## Bayesian network\n",
    "Chow-Liu tree, exact inference, Junction tree, variable elimination, belief propagation, approximate inference, sampling, structural learning, scoring function,\n",
    "    \n",
    "    \n",
    "## K-Nearest neighbours (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Expectation-Maximization (EM)\n",
    "Two steps, E-step and M-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Clustering\n",
    "\n",
    "## K-means\n",
    "Belongs to EM. <br/>\n",
    "K is the hyperparameter selected by hand.\n",
    "\n",
    "### Objective\n",
    "If use one-hot encoding\n",
    "* $ L(r,\\mu) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} r_{ik} ||x_i - \\mu_k|| $\n",
    "* Expectation-Maximization (EM)\n",
    "    * E: $ \\mu_k = \\frac{1}{n_k} \\sum_{i=1}^{n} r_{ik}x_i $\n",
    "    * M: $ r_i = argmin_k ||x_i - \\mu_k|| $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Principle Component Analysis (PCA)\n",
    "\n",
    "基于 Mingkui Tan 的课件 “PCA.pptx”。\n",
    "\n",
    "原本是向量x，找到一个矩阵W，使得新向量z满足 $z = Wx$。相当于旋转坐标系。z,x的维度相同。\n",
    "\n",
    "z的每一维度的重要性是递减的，即$z_1$的重要性大于$z_2$大于$z_3$大于$z_4$......\n",
    "\n",
    "$z_1$是最重要的1维，$z_2$是第二重要的1维......\n",
    "\n",
    "$w^1$，也就是W的第1行，把x投影到$z_1$上，也就是z的第1维。其他$w^i$同理\n",
    "\n",
    "假设现在我们只关注***最重要的1维$z_1$***。\n",
    "\n",
    "对任意一个x，要把x投影到这1维上，那么$z_1 = w^1 x$。**如何找到这个$w^1$？**\n",
    "\n",
    "We want the variance of $z_1$ as large as possible. So, \n",
    "$$w^1 = argmax \\: Var(z_1) = \\frac{1}{N} \\sum_{z_1}(z_1-\\bar{z_1})^2$$\n",
    "subject to $||w^1||_2 = 1$\n",
    "\n",
    "那么$Var(z_1)$是什么？\n",
    "\n",
    "看复习PPT的第10页，最后导出 $Var(z_1) = (w^1)^T Cov(x) (w^1)$。把$Cov(x)$记为S。\n",
    "\n",
    "So, \n",
    "$$w^1 = argmax \\big((w^1)^T S (w^1)\\big)$$\n",
    "subject to $||w^1||_2 = 1$\n",
    "\n",
    "有约束的优化问题就可以用拉格朗日乘子法来解。PPT第11页的$\\alpha$就是Lagrange multiplier。\n",
    "各项求导并令其为零就得到$Sw^1 - \\alpha w^1 = 0$，于是得到红色的等式$Sw^1 = \\alpha w^1$，左右同时左乘一个$(w^1)^T$就得到了$(w^1)^T S (w^1) = \\alpha$。联系前面的等式，so，\n",
    "$$ w^1 = argmax(\\alpha) $$\n",
    "所以choose the maximum one for $\\alpha$。\n",
    "$w^1$就是S的特征向量，对应S的最大特征值$\\lambda_1$。\n",
    "\n",
    "同理，如果我们想要找到$w^2$，那它也是S的特征向量，对应S的第二大特征值$\\lambda_2$。同时，$w^1$ 与 $w^2$ 应该是正交的，相乘为零。\n",
    "\n",
    "The eigenvalues for symmetric matrices are always real. A symmetric n×n real matrix M is said to be positive definite if the scalar $z^TMz$ is positive for every non-zero column vector z of n real numbers.\n",
    "$z^TMz \\geq 0$\n",
    "\n",
    "至于怎么算特征值和特征向量？\n",
    "\n",
    "***忘了***\n",
    "\n",
    "不过WQY说只考概念，所以有可能不用计算？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Recommender System\n",
    "\n",
    "### Model-based Collaborative Filtering\n",
    "\n",
    "m: Number of users\n",
    "\n",
    "n: Number of items\n",
    "\n",
    "k: Number of feature\n",
    "\n",
    "R is m-by-n matrix, P is m-by-k user matrix, Q is k-by-n item matrix.R=PQ\n",
    "\n",
    "Suppose $p$ and $q$ are k-by-1 matries, which are k-dimensional **column** vectors.\n",
    "\n",
    "$P = [p_1, p_2, ..., p_m]^T$\n",
    "\n",
    "$Q = [q_1, q_2, ..., q_m]$\n",
    "\n",
    "Prediction of one element: $\\hat{R_{ui}} = P_{u\\cdot} Q_{\\cdot i} = (p_u)^T q_i$\n",
    "\n",
    "Loss for one element (squared error loss): $L(R_{ui},\\hat{R_{ui}}) = (R_{ui}-\\hat{R_{ui}})^2 = (R_{ui}-(p_u)^T q_i)^2$\n",
    "\n",
    "Loss for the whole P and Q is the sum of loss of each element, plus regulation part: $L = \\sum_{u,i}(R_{ui}-(p_u)^T q_i)^2 + \\lambda ( \\sum_u n_{p_u} ||p_u||_2^2 + \\sum_i n_{q_i} ||q_i||_2^2 )$ \n",
    "\n",
    "### ALS\n",
    "Fixing Q, optimize P, update each $p_u$ as: $p_u \\gets \\sum_i (q_i q_i^T + \\lambda n_{p_u}I)^{-1} Q^T R_{u \\cdot}^T$\n",
    "\n",
    "Fixing P, optimize Q, update each $q_i$ as: $q_i \\gets \\sum_u (p_u p_u^T + \\lambda n_{q_i}I)^{-1} P^T R_{\\cdot i}$\n",
    "\n",
    "ALS is **NOT** scalable to large-scale datasets, but SGD is.\n",
    "\n",
    "### SGD\n",
    "SGD choose the loss function as: $L = \\sum_{u,i}(R_{ui}-(p_u)^T q_i)^2 + ( \\lambda_p ||p_u||_2^2 + \\lambda_q ||q_i||_2^2 )$ \n",
    "* $ E_{ui} = R_{ui} - (p_u)^T q_i $\n",
    "* $ \\frac{\\partial L}{\\partial p_u} = E_{ui}(-q_i) + \\lambda_p p_u $\n",
    "* $ \\frac{\\partial L}{\\partial q_i} = E_{ui}(-p_u) + \\lambda_q q_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent components analysis (ICA)\n",
    "\n",
    "\n",
    "## Reinforcement learning\n",
    "\n",
    "## Neural network and related concepts\n",
    "CNN,  \n",
    "RNN, LSTM,  \n",
    "BP, ReLU  \n",
    "GNN  \n",
    "Auto-encoder  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence\n",
    "\n",
    "Recurrent Neural Network (RNN).\n",
    "\n",
    "Encoder and decoder. Hidden states. Context vector (the encoder’s last hidden state).\n",
    "\n",
    "Input/output elements are characters or words (index). The indexes need to be changed to embeddings.\n",
    "\n",
    "Simple seq2seq does not need a maximum sentence length constraint, but seq2seq with **attention mechanism** needs one.\n",
    "\n",
    "[Pytorch tutorial for seq2seq](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
    "\n",
    "“***Teacher forcing***” is the concept of using the **real target** outputs as each next input, instead of using the **decoder’s guess** as the next input. Using teacher forcing causes it to converge faster but when the trained network is exploited, it may exhibit instability.\n",
    "\n",
    "**Beam search** is often applied to seq2seq model on test stage. It considers multiple solutions at each step. The number of solutions considered simultaneously is controled by the parameter called **beam width**. If the beam width is set to 1, then the beam search is downgraded to greedy search. Here is [an article about beam search](https://medium.com/@dhartidhami/beam-search-in-seq2seq-model-7606d55b21a5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout 可以理解为让模型不要过分依赖某一个特征，因为这个特征随时可能被清除（dropout）。\n",
    "\n",
    "Dropout 的缺点是它的存在使得 loss function 不再是严格定义的了。严格定义的 loss function 在每次 iteration 肯定会往 loss 下降的方向走，引入 dropout 就是引入了随机性，就没法在每次 iteration 都保证 loss 下降。写程序的一个建议就是在调试的时候把 dropout 去掉或者概率设置为 0，然后看看是不是每个 iteration 的 loss 都在下降，是的话就说明模型至少是能正常、正确地工作，然后再设置自己想要的 dropout 概率进行训练。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}