{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes and Equations in Machine Learining\n",
    "Linjian Li <br/>\n",
    "Dec. 27th, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear Regression\n",
    "\n",
    "### Closed-form Solotion\n",
    "* $ L(w) = \\frac{1}{2} ||y-Xw||_2^2 $\n",
    "* $ w^* = argmin_w L(w) = (X^T X)^{-1} x^T y $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Linear Classification\n",
    "\n",
    "### SVM\n",
    "* y = {-1,+1}\n",
    "* $ min_{w,b} \\frac{||w||^2}{2} $ <br/>\n",
    "    s.t. $ y_i (w^T x_i + b) \\geq 1 $\n",
    "* Soft Margin\n",
    "    * $ min_{w,b} \\frac{||w||^2}{2} + \\frac{C}{n} \\ \\sum_{i=1}^{n} \\xi_i $ <br/>\n",
    "        s.t. $ y_i (w^T x_i + b) \\geq 1- \\xi_i $\n",
    "    * hinge loss: $ \\xi_i = max \\big( 0,1-y_i (w^T x_i + b) \\big) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Logistic Regression\n",
    "* sigmoid function: $ g(z) = \\big( 1+exp(-z) \\big)^{-1} $\n",
    "    * $ g'(z) = g(z) \\cdot \\big( 1-g(z) \\big) $\n",
    "* $ h_w(x) = g(w^T x) $ = P(y = +1 | x)\n",
    "* $ J(w) = - \\frac{1}{n} \\bigg[ \\sum_{i=1}^{n} y_i log \\big( h_w(x_i) \\big) + (1-y_i) log \\big( 1-h_w(x_i) \\big) \\bigg] $ <br/>\n",
    "    assume $ y \\in \\{0,1\\} $\n",
    "* Gradient\n",
    "    * For one sample: $ \\frac{\\partial J(w)}{\\partial w} = \\big( h_w(x)-y \\big) x $\n",
    "    * For all sample: $ \\frac{\\partial J(w)}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^{n} \\big( h_w(x_i)-y_i \\big) x_i $\n",
    "\n",
    "---\n",
    "## Softmax Regression\n",
    "* $ P(y_i = j | x) = \\frac{exp(w_j^T x_i)}{\\sum_{l=1}^{K} exp(w_l^T x_i)} $\n",
    "* $ J(w) = - \\frac{1}{n} \\bigg[ \\sum_{i=1}^{n} \\sum_{j=1}^{K} I(y_i = j) log \\frac{exp(w_j^T x_i)}{\\sum_{l=1}^{K} exp(w_l^T x_i)} \\bigg] $\n",
    "* $ \\frac{\\partial J(w)}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^{n} \\bigg[ \\big( P(y_i = j | x_i ; w) - I(y_i = j) \\big) x_i \\bigg] + \\lambda w_j $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ensemble Learning\n",
    "\n",
    "### Decision Tree\n",
    "* $ Entropy(D) = \\sum_{i=1}^{c} -p_i log_2 p_i $\n",
    "* $ Gain(D,A) = Entropy(D)-\\sum_{v \\in Values(A)} \\frac{|D_v|}{D} Entropy(D_v) $\n",
    "\n",
    "### Adaboost\n",
    "* $ w_{m+1}(i) = \\frac{w_{m}(i)}{z_m} exp\\big(-\\alpha_m y_i h_m(x_i)\\big) $\n",
    "* $ z_m = \\sum_{i-1}^{n} exp\\big(-\\alpha_m y_i h_m(x_i)\\big) $\n",
    "* $ \\alpha_m = \\frac{1}{2} log \\frac{1-\\epsilon_m}{\\epsilon_m} $\n",
    "* $ H(x) = \\sum_{m=1}^{M} \\alpha_m h_m(x) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Clustering\n",
    "\n",
    "### K-means\n",
    "* $ L(r,\\mu) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} r_{ik} ||x_i - \\mu_k|| $\n",
    "* Expectation-Maximization (EM)\n",
    "    * E: $ \\mu_k = \\frac{1}{n_k} \\sum_{i=1}^{n} r_{ik}x_i $\n",
    "    * M: $ r_i = argmin_k ||x_i - \\mu_k|| $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Principle Component Analysis (PCA)\n",
    "\n",
    "基于 Mingkui Tan 的课件 “PCA.pptx”。\n",
    "\n",
    "原本是向量x，找到一个矩阵W，使得新向量z满足 $z = Wx$。相当于旋转坐标系。z,x的维度相同。\n",
    "\n",
    "z的每一维度的重要性是递减的，即$z_1$的重要性大于$z_2$大于$z_3$大于$z_4$......\n",
    "\n",
    "$z_1$是最重要的1维，$z_2$是第二重要的1维......\n",
    "\n",
    "$w^1$，也就是W的第1行，把x投影到$z_1$上，也就是z的第1维。其他$w^i$同理\n",
    "\n",
    "假设现在我们只关注***最重要的1维$z_1$***。\n",
    "\n",
    "对任意一个x，要把x投影到这1维上，那么$z_1 = w^1 x$。**如何找到这个$w^1$？**\n",
    "\n",
    "We want the variance of $z_1$ as large as possible. So, \n",
    "$$w^1 = argmax \\: Var(z_1) = \\frac{1}{N} \\sum_{z_1}(z_1-\\bar{z_1})^2$$\n",
    "subject to $||w^1||_2 = 1$\n",
    "\n",
    "那么$Var(z_1)$是什么？\n",
    "\n",
    "看复习PPT的第10页，最后导出 $Var(z_1) = (w^1)^T Cov(x) (w^1)$。把$Cov(x)$记为S。\n",
    "\n",
    "So, \n",
    "$$w^1 = argmax \\big((w^1)^T S (w^1)\\big)$$\n",
    "subject to $||w^1||_2 = 1$\n",
    "\n",
    "有约束的优化问题就可以用拉格朗日乘子法来解。PPT第11页的$\\alpha$就是Lagrange multiplier。\n",
    "各项求导并令其为零就得到$Sw^1 - \\alpha w^1 = 0$，于是得到红色的等式$Sw^1 = \\alpha w^1$，左右同时左乘一个$(w^1)^T$就得到了$(w^1)^T S (w^1) = \\alpha$。联系前面的等式，so，\n",
    "$$ w^1 = argmax(\\alpha) $$\n",
    "所以choose the maximum one for $\\alpha$。\n",
    "$w^1$就是S的特征向量，对应S的最大特征值$\\lambda_1$。\n",
    "\n",
    "同理，如果我们想要找到$w^2$，那它也是S的特征向量，对应S的第二大特征值$\\lambda_2$。同时，$w^1$ 与 $w^2$ 应该是正交的，相乘为零。\n",
    "\n",
    "The eigenvalues for symmetric matrices are always real. A symmetric n×n real matrix M is said to be positive definite if the scalar $z^TMz$ is positive for every non-zero column vector z of n real numbers.\n",
    "$z^TMz \\geq 0$\n",
    "\n",
    "至于怎么算特征值和特征向量？\n",
    "\n",
    "***忘了***\n",
    "\n",
    "不过WQY说只考概念，所以有可能不用计算？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Recommender System\n",
    "\n",
    "### Model-based Collaborative Filtering\n",
    "\n",
    "m: Number of users\n",
    "\n",
    "n: Number of items\n",
    "\n",
    "k: Number of feature\n",
    "\n",
    "R is m-by-n matrix, P is m-by-k user matrix, Q is k-by-n item matrix.R=PQ\n",
    "\n",
    "Suppose $p$ and $q$ are k-by-1 matries, which are k-dimensional **column** vectors.\n",
    "\n",
    "$P = [p_1, p_2, ..., p_m]^T$\n",
    "\n",
    "$Q = [q_1, q_2, ..., q_m]$\n",
    "\n",
    "Prediction of one element: $\\hat{R_{ui}} = P_{u\\cdot} Q_{\\cdot i} = (p_u)^T q_i$\n",
    "\n",
    "Loss for one element (squared error loss): $L(R_{ui},\\hat{R_{ui}}) = (R_{ui}-\\hat{R_{ui}})^2 = (R_{ui}-(p_u)^T q_i)^2$\n",
    "\n",
    "Loss for the whole P and Q is the sum of loss of each element, plus regulation part: $L = \\sum_{u,i}(R_{ui}-(p_u)^T q_i)^2 + \\lambda ( \\sum_u n_{p_u} ||p_u||_2^2 + \\sum_i n_{q_i} ||q_i||_2^2 )$ \n",
    "\n",
    "## ALS\n",
    "Fixing Q, optimize P, update each $p_u$ as: $p_u \\gets (q_i q_i^T + \\lambda n_{p_u}I)^{-1} Q^T R_{u \\cdot}^T$\n",
    "\n",
    "Fixing P, optimize Q, update each $q_i$ as: $q_i \\gets (p_u p_u^T + \\lambda n_{q_i}I)^{-1} P^T R_{\\cdot i}$\n",
    "\n",
    "ALS is **NOT** scalable to large-scale datasets. But SGD is scalable to large-scale datasets.\n",
    "\n",
    "## SGD\n",
    "SGD choose the loss function as: $L = \\sum_{u,i}(R_{ui}-(p_u)^T q_i)^2 + ( \\lambda_p ||p_u||_2^2 + \\lambda_q ||q_i||_2^2 )$ \n",
    "* $ E_{ui} = R_{ui} - (p_u)^T q_i $\n",
    "* $ \\frac{\\partial L}{\\partial p_u} = E_{ui}(-q_i) + \\lambda_p p_u $\n",
    "* $ \\frac{\\partial L}{\\partial q_i} = E_{ui}(-p_u) + \\lambda_q q_i $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
